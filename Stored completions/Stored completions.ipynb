{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00042d91-3903-4161-a4d5-f14254ac72e8",
   "metadata": {},
   "source": [
    "# Stored completions with Azure AI Foundry\n",
    "\n",
    "Stored completions allow you to capture the conversation history from chat completions sessions to use as datasets for evaluations and fine-tuning.\n",
    "\n",
    "> https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/stored-completions?tabs=python-key#stored-completions-api\n",
    "\n",
    "> https://techcommunity.microsoft.com/blog/azure-ai-services-blog/introducing-model-distillation-in-azure-openai-service/4298627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f13bc8a-073e-455f-bc08-b6930fbb10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77f4ce2-89e0-48d9-bb84-7455e4171e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c267cb-33e1-4d0b-a993-80300733354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 18-Apr-2025 07:31:39\n"
     ]
    }
   ],
   "source": [
    "print(f\"Today is {datetime.today().strftime('%d-%b-%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2e0bab-b7bf-4fa9-8ebd-e5f988e6bad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"azure.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8983d187-c306-4999-828e-28d7c70ee193",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_client = AzureOpenAI(api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "                          api_version=\"2025-02-01-preview\",\n",
    "                          azure_endpoint=os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6eff0e-8bf3-4108-8a41-bdf1ea98f87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Ensemble methods in machine learning are strategies used to improve prediction accuracy by combining multiple models. The key techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating)**: This approach involves training multiple models on different random subsets of the data. It aims to reduce variance and avoid overfitting by averaging the predictions. Random Forests are a common application of bagging, where multiple decision trees are built using random subsets of features.\\n\\n2. **Boosting**: Unlike bagging, boosting trains models sequentially, with each model focusing on the errors made by its predecessor. This method reduces bias and increases model accuracy. Gradient Boosting is a specific boosting technique that enhances prediction by iteratively adding models to correct previous predictions.\\n\\n3. **Stacking (Stacked Generalization)**: Stacking combines predictions from various base models using a meta-model to improve overall performance. The meta-model learns how to best integrate predictions from these diverse models to make a final decision.\\n\\nEnsemble methods are powerful because they mitigate overfitting and improve predictive accuracy by leveraging the strengths and compensating for the weaknesses of individual models. They are particularly effective in complex datasets, where capturing different aspects of data is crucial for performance.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = aoai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    store=True,  # True\n",
    "    metadata={\n",
    "        \"user\": \"admin\",\n",
    "        \"category\": \"docs-test\"\n",
    "    },\n",
    "    messages=[{\n",
    "        \"role\":\n",
    "        \"system\",\n",
    "        \"content\":\n",
    "        \"Provide a clear and concise summary of the technical content, highlighting key concepts and their relationships. Focus on the main ideas and practical implications.\"\n",
    "    }, {\n",
    "        \"role\":\n",
    "        \"user\",\n",
    "        \"content\":\n",
    "        \"Ensemble methods combine multiple machine learning models to create a more robust and accurate predictor. Common techniques include bagging (training models on random subsets of data), boosting (sequentially training models to correct previous errors), and stacking (using a meta-model to combine base model predictions). Random Forests, a popular bagging method, create multiple decision trees using random feature subsets. Gradient Boosting builds trees sequentially, with each tree focusing on correcting the errors of previous trees. These methods often achieve better performance than single models by reducing overfitting and variance while capturing different aspects of the data.\"\n",
    "    }])\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a31ec61-9853-4a98-80b2-5f55c0955893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZkWKeTPC3jVafcGnxvtMvk7TJKQ\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods are advanced techniques in machine learning that combine multiple models to enhance predictive accuracy and robustness. The key approaches include:\\n\\n1. **Bagging (Bootstrap Aggregating)**: This involves training models on different random subsets of the training data, aiming to reduce variance and overfitting. A prime example of bagging is Random Forests, where multiple decision trees are created using random subsets of features as well as data, providing stability and improved performance over individual trees.\\n\\n2. **Boosting**: This sequential technique focuses on training models to correct the errors made by previous models. Each subsequent model attempts to address the shortcomings of its predecessors, thus refining and improving the overall prediction. Gradient Boosting is a popular form which constructs trees in sequence, emphasizing areas where errors are made most.\\n\\n3. **Stacking**: This method uses a meta-model to combine the predictions of several base models. The base models provide their predictions, and a higher-level model (the meta-model) learns how to best merge these predictions for enhanced accuracy.\\n\\nEnsemble methods generally outperform single models by reducing overfitting and capturing diverse data aspects, leading to more generalized and reliable predictions. These techniques are particularly useful in complex datasets where singular models might struggle with variance and bias, offering practical benefits in fields requiring high precision such as finance, healthcare, and many areas of data science.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743767024,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 274,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 431,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"7cab15b1-b4c2-4b6d-8a0c-6d7fea7e0aff\",\n",
      "            \"seed\": -5097300764094925597,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZiJl2GSkQ0a1d9LG1AFlnfoZhyN\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning involve combining multiple models to enhance prediction accuracy and robustness. The key techniques used in ensemble methods include:\\n\\n1. **Bagging**: This method involves training multiple models on different random subsets of the dataset. A prominent example is Random Forests, where multiple decision trees are built using random samples and subsets of features to minimize overfitting and variance, thereby improving overall model stability and performance.\\n\\n2. **Boosting**: In boosting, models are trained sequentially, with each new model focusing on correcting the errors of the previous models. Gradient Boosting is a well-known boosting method that builds decision trees one at a time, systematically improving upon the weaknesses of earlier models to achieve high accuracy.\\n\\n3. **Stacking**: This technique involves using a meta-model to integrate predictions from various base models. The meta-model learns how to optimally combine these predictions to further leverage the strengths of diverse models.\\n\\nThe practical implications of ensemble methods are significant: they often outperform single models by effectively reducing overfitting, increasing prediction accuracy, and maintaining robustness. By capturing different data characteristics and reducing error margins, ensemble methods are crucial for achieving high performance in complex prediction tasks.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766887,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 240,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 397,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a494265d-bada-42d3-881e-c31253b048c7\",\n",
      "            \"seed\": -8304168620148096343,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZgKYzU3LO0N08tV1RiVXWpxSpv7\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning involve combining multiple models to enhance prediction accuracy and robustness. The key techniques are bagging, boosting, and stacking:\\n\\n- **Bagging** (Bootstrap Aggregating) involves training models on random subsets of the data to reduce variance and overfitting. Random Forests exemplify bagging by constructing multiple decision trees using varied subsets of features.\\n\\n- **Boosting** focuses on improving model accuracy by sequentially training models where each new model addresses the errors of its predecessor. Gradient Boosting is a popular approach creating trees that progressively correct previous mistakes.\\n\\n- **Stacking** involves training a meta-model to amalgamate predictions from several base models, optimizing overall predictive performance by capturing diverse aspects of the data.\\n\\nThese methods generally outperform individual models by combining their strengths while mitigating weaknesses like overfitting and high variance. Practically, ensemble methods are particularly beneficial in applications requiring high predictive accuracy and reliability.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766764,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 185,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 342,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"7e4b73b0-8410-4b3b-977f-1615844f578d\",\n",
      "            \"seed\": -8080769105501260898,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZf0lObrIOpfFRrcc6hrHZD5mUbK\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning integrate multiple models to enhance prediction accuracy and robustness. The key techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating)**: It involves training models on different random subsets of the data to reduce variance and prevent overfitting. Random Forests are a common application, which combines numerous decision trees built using random subsets of features.\\n\\n2. **Boosting**: This sequential technique aims to improve model accuracy by training each new model to correct errors made by previous ones. Gradient Boosting exemplifies this approach by building trees consecutively, with each new tree focusing on the residuals or errors of the preceding models.\\n\\n3. **Stacking**: Stacking utilizes a meta-model to merge predictions from base models, potentially capturing complex patterns and enhancing overall predictive performance.\\n\\nThese ensemble techniques typically outperform single models by leveraging varied perspectives on the data and balancing bias and variance. The practical implications include better generalization to unseen data, improved stability of predictions, and suitability for complex datasets with intricate relationships.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766682,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 205,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 362,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"5678e6a6-0a34-45b9-893d-e59de38431b8\",\n",
      "            \"seed\": -6399575346861160762,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZbG29dttR70AmKyXEBciiD9tSd5\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance prediction accuracy and robustness by combining multiple models. The key techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating):** It involves training models on random subsets of the data to reduce variance and overfitting. A prominent example is Random Forests, which generate multiple decision trees using random subsets of features, resulting in improved predictive accuracy and stability.\\n\\n2. **Boosting:** This sequential approach trains models iteratively, where each new model aims to correct the errors of the previous ones. Gradient Boosting is a specific technique that builds decision trees one at a time, focusing on residual errors, thus enhancing model precision and reducing bias.\\n\\n3. **Stacking:** It combines predictions from various base models using a meta-model, which learns how to best integrate the diverse predictions, thus capturing different data aspects and improving overall model performance.\\n\\nThese ensemble methods generally outperform single models by effectively decreasing overfitting, variance, and bias, offering more reliable predictions in practical applications.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766450,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 201,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 358,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a962f1c4-22b5-4f84-9ffb-1f55fcbd5a41\",\n",
      "            \"seed\": 3268982671648601920,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZah2M5drUpuo5krz2D0XVF64tsP\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning are techniques that integrate multiple models to enhance prediction accuracy and robustness. The key types of ensemble methods are:\\n\\n1. **Bagging (Bootstrap Aggregating):** This approach involves training several models independently on random subsets of the data. A prototypical example is Random Forests, which build multiple decision trees using random subsets of features. Bagging helps in reducing variance in model predictions and mitigates overfitting by averaging across multiple models.\\n\\n2. **Boosting:** Unlike bagging, boosting trains models sequentially, with each new model focusing on correcting the errors of its predecessors. Gradient Boosting is a common implementation where weak learners, typically shallow decision trees, are added iteratively. Boosting can enhance model performance by adjusting to data anomalies and errors, although it may increase the risk of overfitting if not managed properly.\\n\\n3. **Stacking:** This technique combines the predictions of base models using a meta-model. The base models may be of different types, and their outputs are fed into the meta-model, which learns to make final predictions. Stacking leverages the diverse strengths of various algorithms to improve the overall model accuracy.\\n\\nPractical implications of ensemble methods include improved predictive performance due to the diversity and complementarity of models used. They are particularly useful in complex datasets, providing a mechanism to address high variance and bias in single models, thus offering a more stable and reliable prediction system.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766415,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 288,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 445,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a090fec4-3d65-4ac5-b99d-575f922bfa19\",\n",
      "            \"seed\": -6241145370200263262,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZaeJUEUbilyUbUgm8aZe6Q1sEFH\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning integrate multiple models to enhance prediction accuracy and robustness. The primary techniques include:\\n\\n1. **Bagging**: Involves training models on random subsets of data to reduce variance. A notable example is Random Forests, which generate multiple decision trees using randomly selected features and data samples, improving prediction stability and reducing overfitting.\\n\\n2. **Boosting**: Focuses on sequentially training models to address errors from preceding models. Gradient Boosting is a popular approach where trees are built iteratively, each one correcting its predecessor's mistakes, leading to improved model accuracy.\\n\\n3. **Stacking**: Uses a meta-model to combine predictions from base models. This technique leverages the strengths of different models, usually requiring careful architecture design to optimize outcomes.\\n\\nEnsemble methods generally outperform single-model approaches by balancing bias and variance. They capture diverse data aspects and provide more reliable predictions, making them valuable tools in complex data analysis tasks with practical applications in fields like finance, healthcare, and technology.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766412,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 206,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 363,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"cab4d1a3-071b-4371-97a7-8e6c3f930f68\",\n",
      "            \"seed\": -5226025783387540097,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZabCWVQlnmR36WEnathtpghPOPa\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning involve combining multiple models to enhance prediction accuracy and robustness. Key techniques include:\\n\\n1. **Bagging**: This method involves training multiple models on different random subsets of the data to reduce variance and improve predictions. A prime example is Random Forests, where multiple decision trees are constructed using random subsets of features, mitigating overfitting and ensuring diverse model outputs.\\n\\n2. **Boosting**: Boosting sequentially trains models to focus on correcting the errors of preceding models, effectively reducing bias. Gradient Boosting is a well-known implementation, where each new tree aims to improve upon the mistakes made by the former ones, resulting in a strong, accurate model.\\n\\n3. **Stacking**: This technique uses a meta-model to combine the predictions made by base models. It leverages the strengths of different models to produce a final prediction, typically offering improved performance over using individual models alone.\\n\\nThese ensemble methods work to balance variance, bias, and overfitting, capturing various aspects of the data to enhance predictive performance. Practically, they offer a robust approach for tackling complex datasets in applications needing high accuracy and reliability.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766409,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 231,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 388,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a48f138d-3a68-486b-90aa-febb6ae77db3\",\n",
      "            \"seed\": 5455224915049447703,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZaXH2eBMArdY6re7mKXI8XHUKej\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning are strategies that combine multiple models to enhance prediction accuracy and robustness. The three main approaches are bagging, boosting, and stacking.\\n\\n**Bagging** (Bootstrap Aggregating) involves training multiple models on different random subsets of the data, simultaneously, to reduce variance and prevent overfitting. A key example is Random Forests, which build several decision trees using varied subsets of features, thereby increasing overall stability and accuracy.\\n\\n**Boosting** is a sequential method where each new model specifically aims to correct errors made by previous models. This approach typically results in models that are less prone to bias and have improved performance due to focusing particularly on difficult samples in the dataset. Gradient Boosting is a well-known boosting technique that incrementally improves model predictions by adjusting to errors from prior models.\\n\\n**Stacking** involves training a meta-model to integrate predictions from multiple base models. The meta-model typically learns how to better combine the strengths of the individual models to improve prediction quality.\\n\\nPractical implications of ensemble methods include enhanced predictive accuracy, reduced risk of overfitting, and the ability to capture complex patterns in data that single models may miss. These techniques are widely used in various domains, offering robust solutions for classification and regression problems while allowing for flexibility and improved generalization.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766405,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 259,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 416,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"0cb95439-09a8-4d84-b8ea-aa218dadca07\",\n",
      "            \"seed\": -5892814141565468151,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZa7vvTDa1NpGePfgtf76ReYoBr2\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance predictive performance by combining multiple models. The main techniques include:\\n\\n1. **Bagging**: This involves training multiple models on different random subsets of data to reduce variance and improve stability. Random Forests are a classic example, where numerous decision trees are trained using different random subsets of features.\\n\\n2. **Boosting**: Models are trained sequentially, with each new model focusing on correcting errors made by previous models. This technique reduces bias and improves accuracy. Gradient Boosting is a common approach where subsequent trees emphasize correcting past mistakes, often leading to highly accurate predictions.\\n\\n3. **Stacking**: A meta-model aggregates the predictions from various base models, allowing more complex relationships to be captured, potentially improving the final prediction quality.\\n\\nEnsemble methods typically outperform single models as they reduce overfitting, leverage diverse perspectives from multiple models, and exploit different characteristics of the data. Practically, they are particularly useful in situations requiring high prediction accuracy and robustness, such as financial forecasting, healthcare diagnostics, and natural language processing.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766379,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 213,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 370,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"07734c80-eb13-4c3d-b160-a3401240c483\",\n",
      "            \"seed\": 6650876039918523185,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZa4WoZ6ZmTR4pDGnz1LVwRmO0ZE\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance predictive accuracy by combining multiple models. Key techniques include:\\n\\n1. **Bagging**: This involves training multiple models on different random subsets of the training data. Random Forests are a well-known example, where many decision trees are built using random selections of features, improving robustness and reducing overfitting.\\n\\n2. **Boosting**: This sequentially trains models, with each model attempting to correct the errors of the previous ones. Gradient Boosting is a prominent variant that adjusts weights and builds trees to focus on more difficult cases. This approach reduces variance and increases model precision.\\n\\n3. **Stacking**: Uses a meta-model to aggregate predictions from multiple base models. The meta-model learns the best way to combine these predictions, typically resulting in improved performance.\\n\\nThese ensemble techniques capitalize on different model strengths, leading to superior overall predictions by mitigating issues like overfitting and variance, and capturing diverse data characteristics.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766376,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 191,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 348,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"49b33584-10c7-46c2-ba4d-ad2585c464e2\",\n",
      "            \"seed\": 6751658275219693797,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZa0EbMj29zYDyOIavr2m09wEGAs\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance predictive accuracy by combining multiple models, providing a more robust outcome than individual models. The main techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating):** This involves training multiple models on random subsets of the data, which reduces variance and overfitting. Random Forests are a prime example, where numerous decision trees are built using random subsets of features, contributing to stability and improved performance.\\n\\n2. **Boosting:** Models are trained sequentially, each model correcting the errors of the previous one. Gradient Boosting is a common technique where weak learners (like decision trees) are incrementally improved by focusing on the deficiencies of prior models. This approach enhances the model's ability to capture complex patterns in the data.\\n\\n3. **Stacking (Stacked Generalization):** In this method, predictions from base models are used as inputs for a meta-model, which combines predictions to improve accuracy. This technique leverages the strengths of diverse models by ensembling them into a single powerful predictor.\\n\\nThese ensemble strategies effectively reduce overfitting and model variance, resulting in improved predictions that better capture various aspects of the data. Practical implications include enhanced performance in tasks where data complexity or variability is high, such as in finance or healthcare analytics.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766372,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 255,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 412,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"0c1f4b95-7bb0-4c52-838c-d88684301dae\",\n",
      "            \"seed\": 4240379875835226058,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZZx4vOwsPt8dLKdM0K7oi9ZZkeG\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance predictive accuracy and robustness by integrating multiple models. The key techniques are:\\n\\n1. **Bagging (Bootstrap Aggregating):** Models are trained on random subsets of the dataset, which helps in reducing variance and overfitting. Random Forest is a notable application of bagging, where multiple decision trees are built using random subsets of features.\\n\\n2. **Boosting:** Models are trained sequentially, with each new model focusing on correcting errors made by previous models. This approach enhances performance by reducing bias. Gradient Boosting is a common boosting method which incrementally builds decision trees to refine predictions.\\n\\n3. **Stacking:** This technique combines predictions from base models using a meta-model, which improves overall predictions by leveraging the strengths of individual models.\\n\\nThese ensemble methods generally outperform single models because they integrate various perspectives on the data, effectively balancing bias and variance, reducing overfitting, and capturing complex data patterns. Ensemble approaches have practical implications in scenarios requiring high accuracy and robustness, such as financial forecasting, healthcare diagnostics, and image recognition.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766369,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 214,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 371,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a017d9a4-c995-4c9a-90c2-2b54c6174291\",\n",
      "            \"seed\": 8243908296830242323,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZZsiHP0U9WatwSDay0TUJGFgG6W\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance prediction accuracy and robustness by combining multiple models. The key techniques are:\\n\\n1. **Bagging (Bootstrap Aggregating)**: Models are trained on random subsets of the data. Random Forests, a prominent bagging method, generate numerous decision trees based on random feature subsets, aiming to reduce variance and prevent overfitting.\\n\\n2. **Boosting**: Models are trained sequentially where each model corrects the errors of its predecessor. Gradient Boosting exemplifies this technique by progressively focusing on residual errors, resulting in improved accuracy.\\n\\n3. **Stacking**: This approach involves using a meta-model to synthesize predictions from various base models, potentially leveraging their strengths in different data aspects to enhance overall predictions.\\n\\nEnsemble methods typically outperform single models by leveraging a combination of reduced overfitting, decreased variance, and comprehensive data interpretation. They are widely used to exploit these advantages in practical machine learning applications, achieving higher prediction reliability and accuracy.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766364,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 197,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 354,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"200a3833-f13a-4a66-8f36-66bd6c8ed39c\",\n",
      "            \"seed\": 186743977412696027,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods enhance machine learning performance by combining multiple models to create a more accurate and robust predictor. The key techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating):** It involves training multiple models on different random subsets of data to reduce variance and overfitting. Random Forests are a popular implementation, creating multiple decision trees using random subsets of features.\\n\\n2. **Boosting:** This technique builds models sequentially, each focusing on correcting errors made by previous models. Gradient Boosting is a common form, which incrementally improves predictions by addressing prior inaccuracies.\\n\\n3. **Stacking:** A meta-model is used to combine the predictions from various base models, leveraging their strengths for better performance.\\n\\nThe practical implications of ensemble methods are significant. By utilizing diverse models and approaches, they reduce overfitting and increase generalization, leading to improved accuracy and reliability in predictions compared to single models. These methods are particularly useful in complex datasets where capturing different aspects and dependencies within the data is crucial.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766361,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 201,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 358,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"538a3742-a75b-4772-9cd0-494c402960f0\",\n",
      "            \"seed\": -3153307909102300857,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"has_more\": false,\n",
      "    \"object\": \"list\",\n",
      "    \"total\": 15,\n",
      "    \"first_id\": \"chatcmpl-BIZkWKeTPC3jVafcGnxvtMvk7TJKQ\",\n",
      "    \"last_id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.list()\n",
    "\n",
    "print(response.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "199f8740-05cd-470f-8d9e-f22e5b095abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001: chatcmpl-BIZkWKeTPC3jVafcGnxvtMvk7TJKQ\n",
      "00002: chatcmpl-BIZiJl2GSkQ0a1d9LG1AFlnfoZhyN\n",
      "00003: chatcmpl-BIZgKYzU3LO0N08tV1RiVXWpxSpv7\n",
      "00004: chatcmpl-BIZf0lObrIOpfFRrcc6hrHZD5mUbK\n",
      "00005: chatcmpl-BIZbG29dttR70AmKyXEBciiD9tSd5\n",
      "00006: chatcmpl-BIZah2M5drUpuo5krz2D0XVF64tsP\n",
      "00007: chatcmpl-BIZaeJUEUbilyUbUgm8aZe6Q1sEFH\n",
      "00008: chatcmpl-BIZabCWVQlnmR36WEnathtpghPOPa\n",
      "00009: chatcmpl-BIZaXH2eBMArdY6re7mKXI8XHUKej\n",
      "00010: chatcmpl-BIZa7vvTDa1NpGePfgtf76ReYoBr2\n",
      "00011: chatcmpl-BIZa4WoZ6ZmTR4pDGnz1LVwRmO0ZE\n",
      "00012: chatcmpl-BIZa0EbMj29zYDyOIavr2m09wEGAs\n",
      "00013: chatcmpl-BIZZx4vOwsPt8dLKdM0K7oi9ZZkeG\n",
      "00014: chatcmpl-BIZZsiHP0U9WatwSDay0TUJGFgG6W\n",
      "00015: chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.list()\n",
    "\n",
    "for index, item in enumerate(response.data, start=1):\n",
    "    print(f\"{index:05}: {item.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bbe3ef8-83f7-4e4f-ae3d-db262fcd7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idexample = \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e215438a-aa77-45eb-83fa-0cd6714699c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": null,\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"Ensemble methods enhance machine learning performance by combining multiple models to create a more accurate and robust predictor. The key techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating):** It involves training multiple models on different random subsets of data to reduce variance and overfitting. Random Forests are a popular implementation, creating multiple decision trees using random subsets of features.\\n\\n2. **Boosting:** This technique builds models sequentially, each focusing on correcting errors made by previous models. Gradient Boosting is a common form, which incrementally improves predictions by addressing prior inaccuracies.\\n\\n3. **Stacking:** A meta-model is used to combine the predictions from various base models, leveraging their strengths for better performance.\\n\\nThe practical implications of ensemble methods are significant. By utilizing diverse models and approaches, they reduce overfitting and increase generalization, leading to improved accuracy and reliability in predictions compared to single models. These methods are particularly useful in complex datasets where capturing different aspects and dependencies within the data is crucial.\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"annotations\": null,\n",
      "                \"audio\": null,\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1743766361,\n",
      "    \"model\": \"gpt-4o-2024-08-06\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 201,\n",
      "        \"prompt_tokens\": 157,\n",
      "        \"total_tokens\": 358,\n",
      "        \"completion_tokens_details\": null,\n",
      "        \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"request_id\": \"538a3742-a75b-4772-9cd0-494c402960f0\",\n",
      "    \"seed\": -3153307909102300857,\n",
      "    \"top_p\": 1,\n",
      "    \"temperature\": 1,\n",
      "    \"presence_penalty\": 0,\n",
      "    \"frequency_penalty\": 0,\n",
      "    \"metadata\": {\n",
      "        \"user\": \"admin\",\n",
      "        \"category\": \"docs-test\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.retrieve(idexample)\n",
    "\n",
    "print(response.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe376181-a49a-4cab-9666-3ca4a474645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"content\": \"Provide a clear and concise summary of the technical content, highlighting key concepts and their relationships. Focus on the main ideas and practical implications.\",\n",
      "      \"refusal\": null,\n",
      "      \"role\": \"system\",\n",
      "      \"annotations\": null,\n",
      "      \"audio\": null,\n",
      "      \"function_call\": null,\n",
      "      \"tool_calls\": null,\n",
      "      \"id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2-0\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"Ensemble methods combine multiple machine learning models to create a more robust and accurate predictor. Common techniques include bagging (training models on random subsets of data), boosting (sequentially training models to correct previous errors), and stacking (using a meta-model to combine base model predictions). Random Forests, a popular bagging method, create multiple decision trees using random feature subsets. Gradient Boosting builds trees sequentially, with each tree focusing on correcting the errors of previous trees. These methods often achieve better performance than single models by reducing overfitting and variance while capturing different aspects of the data.\",\n",
      "      \"refusal\": null,\n",
      "      \"role\": \"user\",\n",
      "      \"annotations\": null,\n",
      "      \"audio\": null,\n",
      "      \"function_call\": null,\n",
      "      \"tool_calls\": null,\n",
      "      \"id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2-1\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_more\": false,\n",
      "  \"object\": \"list\",\n",
      "  \"total\": 2,\n",
      "  \"first_id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2-0\",\n",
      "  \"last_id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2-1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.messages.list(idexample, limit=4)\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac13bdbf-6b0f-4268-9804-918e4b8d2436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZkWKeTPC3jVafcGnxvtMvk7TJKQ\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods are advanced techniques in machine learning that combine multiple models to enhance predictive accuracy and robustness. The key approaches include:\\n\\n1. **Bagging (Bootstrap Aggregating)**: This involves training models on different random subsets of the training data, aiming to reduce variance and overfitting. A prime example of bagging is Random Forests, where multiple decision trees are created using random subsets of features as well as data, providing stability and improved performance over individual trees.\\n\\n2. **Boosting**: This sequential technique focuses on training models to correct the errors made by previous models. Each subsequent model attempts to address the shortcomings of its predecessors, thus refining and improving the overall prediction. Gradient Boosting is a popular form which constructs trees in sequence, emphasizing areas where errors are made most.\\n\\n3. **Stacking**: This method uses a meta-model to combine the predictions of several base models. The base models provide their predictions, and a higher-level model (the meta-model) learns how to best merge these predictions for enhanced accuracy.\\n\\nEnsemble methods generally outperform single models by reducing overfitting and capturing diverse data aspects, leading to more generalized and reliable predictions. These techniques are particularly useful in complex datasets where singular models might struggle with variance and bias, offering practical benefits in fields requiring high precision such as finance, healthcare, and many areas of data science.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743767024,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 274,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 431,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"7cab15b1-b4c2-4b6d-8a0c-6d7fea7e0aff\",\n",
      "            \"seed\": -5097300764094925597,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZiJl2GSkQ0a1d9LG1AFlnfoZhyN\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning involve combining multiple models to enhance prediction accuracy and robustness. The key techniques used in ensemble methods include:\\n\\n1. **Bagging**: This method involves training multiple models on different random subsets of the dataset. A prominent example is Random Forests, where multiple decision trees are built using random samples and subsets of features to minimize overfitting and variance, thereby improving overall model stability and performance.\\n\\n2. **Boosting**: In boosting, models are trained sequentially, with each new model focusing on correcting the errors of the previous models. Gradient Boosting is a well-known boosting method that builds decision trees one at a time, systematically improving upon the weaknesses of earlier models to achieve high accuracy.\\n\\n3. **Stacking**: This technique involves using a meta-model to integrate predictions from various base models. The meta-model learns how to optimally combine these predictions to further leverage the strengths of diverse models.\\n\\nThe practical implications of ensemble methods are significant: they often outperform single models by effectively reducing overfitting, increasing prediction accuracy, and maintaining robustness. By capturing different data characteristics and reducing error margins, ensemble methods are crucial for achieving high performance in complex prediction tasks.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766887,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 240,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 397,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a494265d-bada-42d3-881e-c31253b048c7\",\n",
      "            \"seed\": -8304168620148096343,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZgKYzU3LO0N08tV1RiVXWpxSpv7\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning involve combining multiple models to enhance prediction accuracy and robustness. The key techniques are bagging, boosting, and stacking:\\n\\n- **Bagging** (Bootstrap Aggregating) involves training models on random subsets of the data to reduce variance and overfitting. Random Forests exemplify bagging by constructing multiple decision trees using varied subsets of features.\\n\\n- **Boosting** focuses on improving model accuracy by sequentially training models where each new model addresses the errors of its predecessor. Gradient Boosting is a popular approach creating trees that progressively correct previous mistakes.\\n\\n- **Stacking** involves training a meta-model to amalgamate predictions from several base models, optimizing overall predictive performance by capturing diverse aspects of the data.\\n\\nThese methods generally outperform individual models by combining their strengths while mitigating weaknesses like overfitting and high variance. Practically, ensemble methods are particularly beneficial in applications requiring high predictive accuracy and reliability.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766764,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 185,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 342,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"7e4b73b0-8410-4b3b-977f-1615844f578d\",\n",
      "            \"seed\": -8080769105501260898,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZf0lObrIOpfFRrcc6hrHZD5mUbK\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning integrate multiple models to enhance prediction accuracy and robustness. The key techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating)**: It involves training models on different random subsets of the data to reduce variance and prevent overfitting. Random Forests are a common application, which combines numerous decision trees built using random subsets of features.\\n\\n2. **Boosting**: This sequential technique aims to improve model accuracy by training each new model to correct errors made by previous ones. Gradient Boosting exemplifies this approach by building trees consecutively, with each new tree focusing on the residuals or errors of the preceding models.\\n\\n3. **Stacking**: Stacking utilizes a meta-model to merge predictions from base models, potentially capturing complex patterns and enhancing overall predictive performance.\\n\\nThese ensemble techniques typically outperform single models by leveraging varied perspectives on the data and balancing bias and variance. The practical implications include better generalization to unseen data, improved stability of predictions, and suitability for complex datasets with intricate relationships.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766682,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 205,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 362,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"5678e6a6-0a34-45b9-893d-e59de38431b8\",\n",
      "            \"seed\": -6399575346861160762,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZbG29dttR70AmKyXEBciiD9tSd5\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance prediction accuracy and robustness by combining multiple models. The key techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating):** It involves training models on random subsets of the data to reduce variance and overfitting. A prominent example is Random Forests, which generate multiple decision trees using random subsets of features, resulting in improved predictive accuracy and stability.\\n\\n2. **Boosting:** This sequential approach trains models iteratively, where each new model aims to correct the errors of the previous ones. Gradient Boosting is a specific technique that builds decision trees one at a time, focusing on residual errors, thus enhancing model precision and reducing bias.\\n\\n3. **Stacking:** It combines predictions from various base models using a meta-model, which learns how to best integrate the diverse predictions, thus capturing different data aspects and improving overall model performance.\\n\\nThese ensemble methods generally outperform single models by effectively decreasing overfitting, variance, and bias, offering more reliable predictions in practical applications.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766450,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 201,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 358,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a962f1c4-22b5-4f84-9ffb-1f55fcbd5a41\",\n",
      "            \"seed\": 3268982671648601920,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZah2M5drUpuo5krz2D0XVF64tsP\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning are techniques that integrate multiple models to enhance prediction accuracy and robustness. The key types of ensemble methods are:\\n\\n1. **Bagging (Bootstrap Aggregating):** This approach involves training several models independently on random subsets of the data. A prototypical example is Random Forests, which build multiple decision trees using random subsets of features. Bagging helps in reducing variance in model predictions and mitigates overfitting by averaging across multiple models.\\n\\n2. **Boosting:** Unlike bagging, boosting trains models sequentially, with each new model focusing on correcting the errors of its predecessors. Gradient Boosting is a common implementation where weak learners, typically shallow decision trees, are added iteratively. Boosting can enhance model performance by adjusting to data anomalies and errors, although it may increase the risk of overfitting if not managed properly.\\n\\n3. **Stacking:** This technique combines the predictions of base models using a meta-model. The base models may be of different types, and their outputs are fed into the meta-model, which learns to make final predictions. Stacking leverages the diverse strengths of various algorithms to improve the overall model accuracy.\\n\\nPractical implications of ensemble methods include improved predictive performance due to the diversity and complementarity of models used. They are particularly useful in complex datasets, providing a mechanism to address high variance and bias in single models, thus offering a more stable and reliable prediction system.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766415,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 288,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 445,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a090fec4-3d65-4ac5-b99d-575f922bfa19\",\n",
      "            \"seed\": -6241145370200263262,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZaeJUEUbilyUbUgm8aZe6Q1sEFH\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning integrate multiple models to enhance prediction accuracy and robustness. The primary techniques include:\\n\\n1. **Bagging**: Involves training models on random subsets of data to reduce variance. A notable example is Random Forests, which generate multiple decision trees using randomly selected features and data samples, improving prediction stability and reducing overfitting.\\n\\n2. **Boosting**: Focuses on sequentially training models to address errors from preceding models. Gradient Boosting is a popular approach where trees are built iteratively, each one correcting its predecessor's mistakes, leading to improved model accuracy.\\n\\n3. **Stacking**: Uses a meta-model to combine predictions from base models. This technique leverages the strengths of different models, usually requiring careful architecture design to optimize outcomes.\\n\\nEnsemble methods generally outperform single-model approaches by balancing bias and variance. They capture diverse data aspects and provide more reliable predictions, making them valuable tools in complex data analysis tasks with practical applications in fields like finance, healthcare, and technology.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766412,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 206,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 363,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"cab4d1a3-071b-4371-97a7-8e6c3f930f68\",\n",
      "            \"seed\": -5226025783387540097,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZabCWVQlnmR36WEnathtpghPOPa\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning involve combining multiple models to enhance prediction accuracy and robustness. Key techniques include:\\n\\n1. **Bagging**: This method involves training multiple models on different random subsets of the data to reduce variance and improve predictions. A prime example is Random Forests, where multiple decision trees are constructed using random subsets of features, mitigating overfitting and ensuring diverse model outputs.\\n\\n2. **Boosting**: Boosting sequentially trains models to focus on correcting the errors of preceding models, effectively reducing bias. Gradient Boosting is a well-known implementation, where each new tree aims to improve upon the mistakes made by the former ones, resulting in a strong, accurate model.\\n\\n3. **Stacking**: This technique uses a meta-model to combine the predictions made by base models. It leverages the strengths of different models to produce a final prediction, typically offering improved performance over using individual models alone.\\n\\nThese ensemble methods work to balance variance, bias, and overfitting, capturing various aspects of the data to enhance predictive performance. Practically, they offer a robust approach for tackling complex datasets in applications needing high accuracy and reliability.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766409,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 231,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 388,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a48f138d-3a68-486b-90aa-febb6ae77db3\",\n",
      "            \"seed\": 5455224915049447703,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZaXH2eBMArdY6re7mKXI8XHUKej\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning are strategies that combine multiple models to enhance prediction accuracy and robustness. The three main approaches are bagging, boosting, and stacking.\\n\\n**Bagging** (Bootstrap Aggregating) involves training multiple models on different random subsets of the data, simultaneously, to reduce variance and prevent overfitting. A key example is Random Forests, which build several decision trees using varied subsets of features, thereby increasing overall stability and accuracy.\\n\\n**Boosting** is a sequential method where each new model specifically aims to correct errors made by previous models. This approach typically results in models that are less prone to bias and have improved performance due to focusing particularly on difficult samples in the dataset. Gradient Boosting is a well-known boosting technique that incrementally improves model predictions by adjusting to errors from prior models.\\n\\n**Stacking** involves training a meta-model to integrate predictions from multiple base models. The meta-model typically learns how to better combine the strengths of the individual models to improve prediction quality.\\n\\nPractical implications of ensemble methods include enhanced predictive accuracy, reduced risk of overfitting, and the ability to capture complex patterns in data that single models may miss. These techniques are widely used in various domains, offering robust solutions for classification and regression problems while allowing for flexibility and improved generalization.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766405,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 259,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 416,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"0cb95439-09a8-4d84-b8ea-aa218dadca07\",\n",
      "            \"seed\": -5892814141565468151,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZa7vvTDa1NpGePfgtf76ReYoBr2\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance predictive performance by combining multiple models. The main techniques include:\\n\\n1. **Bagging**: This involves training multiple models on different random subsets of data to reduce variance and improve stability. Random Forests are a classic example, where numerous decision trees are trained using different random subsets of features.\\n\\n2. **Boosting**: Models are trained sequentially, with each new model focusing on correcting errors made by previous models. This technique reduces bias and improves accuracy. Gradient Boosting is a common approach where subsequent trees emphasize correcting past mistakes, often leading to highly accurate predictions.\\n\\n3. **Stacking**: A meta-model aggregates the predictions from various base models, allowing more complex relationships to be captured, potentially improving the final prediction quality.\\n\\nEnsemble methods typically outperform single models as they reduce overfitting, leverage diverse perspectives from multiple models, and exploit different characteristics of the data. Practically, they are particularly useful in situations requiring high prediction accuracy and robustness, such as financial forecasting, healthcare diagnostics, and natural language processing.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766379,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 213,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 370,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"07734c80-eb13-4c3d-b160-a3401240c483\",\n",
      "            \"seed\": 6650876039918523185,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZa4WoZ6ZmTR4pDGnz1LVwRmO0ZE\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance predictive accuracy by combining multiple models. Key techniques include:\\n\\n1. **Bagging**: This involves training multiple models on different random subsets of the training data. Random Forests are a well-known example, where many decision trees are built using random selections of features, improving robustness and reducing overfitting.\\n\\n2. **Boosting**: This sequentially trains models, with each model attempting to correct the errors of the previous ones. Gradient Boosting is a prominent variant that adjusts weights and builds trees to focus on more difficult cases. This approach reduces variance and increases model precision.\\n\\n3. **Stacking**: Uses a meta-model to aggregate predictions from multiple base models. The meta-model learns the best way to combine these predictions, typically resulting in improved performance.\\n\\nThese ensemble techniques capitalize on different model strengths, leading to superior overall predictions by mitigating issues like overfitting and variance, and capturing diverse data characteristics.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766376,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 191,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 348,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"49b33584-10c7-46c2-ba4d-ad2585c464e2\",\n",
      "            \"seed\": 6751658275219693797,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZa0EbMj29zYDyOIavr2m09wEGAs\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance predictive accuracy by combining multiple models, providing a more robust outcome than individual models. The main techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating):** This involves training multiple models on random subsets of the data, which reduces variance and overfitting. Random Forests are a prime example, where numerous decision trees are built using random subsets of features, contributing to stability and improved performance.\\n\\n2. **Boosting:** Models are trained sequentially, each model correcting the errors of the previous one. Gradient Boosting is a common technique where weak learners (like decision trees) are incrementally improved by focusing on the deficiencies of prior models. This approach enhances the model's ability to capture complex patterns in the data.\\n\\n3. **Stacking (Stacked Generalization):** In this method, predictions from base models are used as inputs for a meta-model, which combines predictions to improve accuracy. This technique leverages the strengths of diverse models by ensembling them into a single powerful predictor.\\n\\nThese ensemble strategies effectively reduce overfitting and model variance, resulting in improved predictions that better capture various aspects of the data. Practical implications include enhanced performance in tasks where data complexity or variability is high, such as in finance or healthcare analytics.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766372,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 255,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 412,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"0c1f4b95-7bb0-4c52-838c-d88684301dae\",\n",
      "            \"seed\": 4240379875835226058,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZZx4vOwsPt8dLKdM0K7oi9ZZkeG\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance predictive accuracy and robustness by integrating multiple models. The key techniques are:\\n\\n1. **Bagging (Bootstrap Aggregating):** Models are trained on random subsets of the dataset, which helps in reducing variance and overfitting. Random Forest is a notable application of bagging, where multiple decision trees are built using random subsets of features.\\n\\n2. **Boosting:** Models are trained sequentially, with each new model focusing on correcting errors made by previous models. This approach enhances performance by reducing bias. Gradient Boosting is a common boosting method which incrementally builds decision trees to refine predictions.\\n\\n3. **Stacking:** This technique combines predictions from base models using a meta-model, which improves overall predictions by leveraging the strengths of individual models.\\n\\nThese ensemble methods generally outperform single models because they integrate various perspectives on the data, effectively balancing bias and variance, reducing overfitting, and capturing complex data patterns. Ensemble approaches have practical implications in scenarios requiring high accuracy and robustness, such as financial forecasting, healthcare diagnostics, and image recognition.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766369,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 214,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 371,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"a017d9a4-c995-4c9a-90c2-2b54c6174291\",\n",
      "            \"seed\": 8243908296830242323,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZZsiHP0U9WatwSDay0TUJGFgG6W\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods in machine learning enhance prediction accuracy and robustness by combining multiple models. The key techniques are:\\n\\n1. **Bagging (Bootstrap Aggregating)**: Models are trained on random subsets of the data. Random Forests, a prominent bagging method, generate numerous decision trees based on random feature subsets, aiming to reduce variance and prevent overfitting.\\n\\n2. **Boosting**: Models are trained sequentially where each model corrects the errors of its predecessor. Gradient Boosting exemplifies this technique by progressively focusing on residual errors, resulting in improved accuracy.\\n\\n3. **Stacking**: This approach involves using a meta-model to synthesize predictions from various base models, potentially leveraging their strengths in different data aspects to enhance overall predictions.\\n\\nEnsemble methods typically outperform single models by leveraging a combination of reduced overfitting, decreased variance, and comprehensive data interpretation. They are widely used to exploit these advantages in practical machine learning applications, achieving higher prediction reliability and accuracy.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766364,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 197,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 354,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"200a3833-f13a-4a66-8f36-66bd6c8ed39c\",\n",
      "            \"seed\": 186743977412696027,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2\",\n",
      "            \"choices\": [\n",
      "                {\n",
      "                    \"finish_reason\": null,\n",
      "                    \"index\": 0,\n",
      "                    \"logprobs\": null,\n",
      "                    \"message\": {\n",
      "                        \"content\": \"Ensemble methods enhance machine learning performance by combining multiple models to create a more accurate and robust predictor. The key techniques include:\\n\\n1. **Bagging (Bootstrap Aggregating):** It involves training multiple models on different random subsets of data to reduce variance and overfitting. Random Forests are a popular implementation, creating multiple decision trees using random subsets of features.\\n\\n2. **Boosting:** This technique builds models sequentially, each focusing on correcting errors made by previous models. Gradient Boosting is a common form, which incrementally improves predictions by addressing prior inaccuracies.\\n\\n3. **Stacking:** A meta-model is used to combine the predictions from various base models, leveraging their strengths for better performance.\\n\\nThe practical implications of ensemble methods are significant. By utilizing diverse models and approaches, they reduce overfitting and increase generalization, leading to improved accuracy and reliability in predictions compared to single models. These methods are particularly useful in complex datasets where capturing different aspects and dependencies within the data is crucial.\",\n",
      "                        \"refusal\": null,\n",
      "                        \"role\": \"assistant\",\n",
      "                        \"annotations\": null,\n",
      "                        \"audio\": null,\n",
      "                        \"function_call\": null,\n",
      "                        \"tool_calls\": null\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"created\": 1743766361,\n",
      "            \"model\": \"gpt-4o-2024-08-06\",\n",
      "            \"object\": null,\n",
      "            \"service_tier\": null,\n",
      "            \"system_fingerprint\": \"fp_ee1d74bde0\",\n",
      "            \"usage\": {\n",
      "                \"completion_tokens\": 201,\n",
      "                \"prompt_tokens\": 157,\n",
      "                \"total_tokens\": 358,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "            },\n",
      "            \"request_id\": \"538a3742-a75b-4772-9cd0-494c402960f0\",\n",
      "            \"seed\": -3153307909102300857,\n",
      "            \"top_p\": 1,\n",
      "            \"temperature\": 1,\n",
      "            \"presence_penalty\": 0,\n",
      "            \"frequency_penalty\": 0,\n",
      "            \"metadata\": {\n",
      "                \"user\": \"admin\",\n",
      "                \"category\": \"docs-test\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"has_more\": false,\n",
      "    \"object\": \"list\",\n",
      "    \"total\": 15,\n",
      "    \"first_id\": \"chatcmpl-BIZkWKeTPC3jVafcGnxvtMvk7TJKQ\",\n",
      "    \"last_id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.list()\n",
    "\n",
    "print(response.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1bea021-23bb-4682-9894-81931e7ad5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001: chatcmpl-BIZkWKeTPC3jVafcGnxvtMvk7TJKQ\n",
      "00002: chatcmpl-BIZiJl2GSkQ0a1d9LG1AFlnfoZhyN\n",
      "00003: chatcmpl-BIZgKYzU3LO0N08tV1RiVXWpxSpv7\n",
      "00004: chatcmpl-BIZf0lObrIOpfFRrcc6hrHZD5mUbK\n",
      "00005: chatcmpl-BIZbG29dttR70AmKyXEBciiD9tSd5\n",
      "00006: chatcmpl-BIZah2M5drUpuo5krz2D0XVF64tsP\n",
      "00007: chatcmpl-BIZaeJUEUbilyUbUgm8aZe6Q1sEFH\n",
      "00008: chatcmpl-BIZabCWVQlnmR36WEnathtpghPOPa\n",
      "00009: chatcmpl-BIZaXH2eBMArdY6re7mKXI8XHUKej\n",
      "00010: chatcmpl-BIZa7vvTDa1NpGePfgtf76ReYoBr2\n",
      "00011: chatcmpl-BIZa4WoZ6ZmTR4pDGnz1LVwRmO0ZE\n",
      "00012: chatcmpl-BIZa0EbMj29zYDyOIavr2m09wEGAs\n",
      "00013: chatcmpl-BIZZx4vOwsPt8dLKdM0K7oi9ZZkeG\n",
      "00014: chatcmpl-BIZZsiHP0U9WatwSDay0TUJGFgG6W\n",
      "00015: chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.list()\n",
    "\n",
    "for index, item in enumerate(response.data, start=1):\n",
    "    print(f\"{index:05}: {item.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f75cce77-c994-4b5f-a0af-17a78aeda7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-BIZZpteC7DubJUWXuKXbWvzBz02y2\",\n",
      "  \"deleted\": true,\n",
      "  \"object\": \"chat.completion.deleted\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.delete(idexample)\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2b378a1-333c-4f82-b018-3029f4b5d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001: chatcmpl-BIZkWKeTPC3jVafcGnxvtMvk7TJKQ\n",
      "00002: chatcmpl-BIZiJl2GSkQ0a1d9LG1AFlnfoZhyN\n",
      "00003: chatcmpl-BIZgKYzU3LO0N08tV1RiVXWpxSpv7\n",
      "00004: chatcmpl-BIZf0lObrIOpfFRrcc6hrHZD5mUbK\n",
      "00005: chatcmpl-BIZbG29dttR70AmKyXEBciiD9tSd5\n",
      "00006: chatcmpl-BIZah2M5drUpuo5krz2D0XVF64tsP\n",
      "00007: chatcmpl-BIZaeJUEUbilyUbUgm8aZe6Q1sEFH\n",
      "00008: chatcmpl-BIZabCWVQlnmR36WEnathtpghPOPa\n",
      "00009: chatcmpl-BIZaXH2eBMArdY6re7mKXI8XHUKej\n",
      "00010: chatcmpl-BIZa7vvTDa1NpGePfgtf76ReYoBr2\n",
      "00011: chatcmpl-BIZa4WoZ6ZmTR4pDGnz1LVwRmO0ZE\n",
      "00012: chatcmpl-BIZa0EbMj29zYDyOIavr2m09wEGAs\n",
      "00013: chatcmpl-BIZZx4vOwsPt8dLKdM0K7oi9ZZkeG\n",
      "00014: chatcmpl-BIZZsiHP0U9WatwSDay0TUJGFgG6W\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.list()\n",
    "\n",
    "for index, item in enumerate(response.data, start=1):\n",
    "    print(f\"{index:05}: {item.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d92d622d-0b74-4370-8c77-eef371fedff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5e089-d4b9-4966-adfe-d3fc0a64043a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
