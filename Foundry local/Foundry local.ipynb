{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee623337",
   "metadata": {},
   "source": [
    "# Foundry local\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/what-is-foundry-local\n",
    "\n",
    "Foundry Local is available in preview. Public preview releases provide early access to features that are in active deployment.\n",
    "Features, approaches, and processes can change or have limited capabilities, before General Availability (GA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be3e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install foundry-local-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80582ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from foundry_local import FoundryLocalManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b627423",
   "metadata": {},
   "source": [
    "## List of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47367e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = FoundryLocalManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a16e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models in the catalog: [FoundryModelInfo(alias=phi-4, id=Phi-4-cuda-gpu, runtime=cuda, file_size=8570 MB, license=MIT), FoundryModelInfo(alias=phi-4, id=Phi-4-generic-gpu, runtime=webgpu, file_size=8570 MB, license=MIT), FoundryModelInfo(alias=phi-4, id=Phi-4-generic-cpu, runtime=cpu, file_size=10403 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-128k, id=Phi-3-mini-128k-instruct-cuda-gpu, runtime=cuda, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-128k, id=Phi-3-mini-128k-instruct-generic-gpu, runtime=webgpu, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-128k, id=Phi-3-mini-128k-instruct-generic-cpu, runtime=cpu, file_size=2600 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-cuda-gpu, runtime=cuda, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-generic-gpu, runtime=webgpu, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-generic-cpu, runtime=cpu, file_size=2590 MB, license=MIT), FoundryModelInfo(alias=mistral-7b-v0.2, id=mistralai-Mistral-7B-Instruct-v0-2-cuda-gpu, runtime=cuda, file_size=4075 MB, license=apache-2.0), FoundryModelInfo(alias=mistral-7b-v0.2, id=mistralai-Mistral-7B-Instruct-v0-2-generic-gpu, runtime=webgpu, file_size=4167 MB, license=apache-2.0), FoundryModelInfo(alias=mistral-7b-v0.2, id=mistralai-Mistral-7B-Instruct-v0-2-generic-cpu, runtime=cpu, file_size=4167 MB, license=apache-2.0), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu, runtime=cuda, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-gpu, runtime=webgpu, file_size=2211 MB, license=MIT), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-cpu, runtime=cpu, file_size=2590 MB, license=MIT), FoundryModelInfo(alias=deepseek-r1-14b, id=deepseek-r1-distill-qwen-14b-cuda-gpu, runtime=cuda, file_size=10065 MB, license=MIT), FoundryModelInfo(alias=deepseek-r1-14b, id=deepseek-r1-distill-qwen-14b-generic-gpu, runtime=webgpu, file_size=10516 MB, license=MIT), FoundryModelInfo(alias=deepseek-r1-14b, id=deepseek-r1-distill-qwen-14b-generic-cpu, runtime=cpu, file_size=11786 MB, license=MIT), FoundryModelInfo(alias=deepseek-r1-7b, id=deepseek-r1-distill-qwen-7b-cuda-gpu, runtime=cuda, file_size=5406 MB, license=MIT), FoundryModelInfo(alias=deepseek-r1-7b, id=deepseek-r1-distill-qwen-7b-generic-gpu, runtime=webgpu, file_size=5713 MB, license=MIT), FoundryModelInfo(alias=deepseek-r1-7b, id=deepseek-r1-distill-qwen-7b-generic-cpu, runtime=cpu, file_size=6584 MB, license=MIT), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-cuda-gpu, runtime=cuda, file_size=528 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-generic-gpu, runtime=webgpu, file_size=700 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-generic-cpu, runtime=cpu, file_size=822 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-1.5b, id=qwen2.5-1.5b-instruct-cuda-gpu, runtime=cuda, file_size=1280 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-1.5b, id=qwen2.5-1.5b-instruct-generic-gpu, runtime=webgpu, file_size=1546 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-1.5b, id=qwen2.5-1.5b-instruct-generic-cpu, runtime=cpu, file_size=1822 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-cuda-gpu, runtime=cuda, file_size=4843 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-generic-gpu, runtime=webgpu, file_size=4843 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-generic-cpu, runtime=cpu, file_size=6307 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-0.5b, id=qwen2.5-coder-0.5b-instruct-cuda-gpu, runtime=cuda, file_size=528 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-0.5b, id=qwen2.5-coder-0.5b-instruct-generic-gpu, runtime=webgpu, file_size=528 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-0.5b, id=qwen2.5-coder-0.5b-instruct-generic-cpu, runtime=cpu, file_size=822 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-1.5b, id=qwen2.5-coder-1.5b-instruct-cuda-gpu, runtime=cuda, file_size=1280 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-1.5b, id=qwen2.5-coder-1.5b-instruct-generic-gpu, runtime=webgpu, file_size=1280 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-1.5b, id=qwen2.5-coder-1.5b-instruct-generic-cpu, runtime=cpu, file_size=1822 MB, license=apache-2.0), FoundryModelInfo(alias=phi-4-mini, id=Phi-4-mini-instruct-cuda-gpu, runtime=cuda, file_size=3686 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini, id=Phi-4-mini-instruct-generic-gpu, runtime=webgpu, file_size=3809 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini, id=Phi-4-mini-instruct-generic-cpu, runtime=cpu, file_size=4915 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini-reasoning, id=Phi-4-mini-reasoning-cuda-gpu, runtime=cuda, file_size=3225 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini-reasoning, id=Phi-4-mini-reasoning-generic-gpu, runtime=webgpu, file_size=3225 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini-reasoning, id=Phi-4-mini-reasoning-generic-cpu, runtime=cpu, file_size=4628 MB, license=MIT), FoundryModelInfo(alias=qwen2.5-14b, id=qwen2.5-14b-instruct-cuda-gpu, runtime=cuda, file_size=9000 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-14b, id=qwen2.5-14b-instruct-generic-cpu, runtime=cpu, file_size=11325 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-cuda-gpu, runtime=cuda, file_size=4843 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-generic-gpu, runtime=webgpu, file_size=5324 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-generic-cpu, runtime=cpu, file_size=6307 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-14b, id=qwen2.5-coder-14b-instruct-cuda-gpu, runtime=cuda, file_size=9000 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-14b, id=qwen2.5-coder-14b-instruct-generic-gpu, runtime=webgpu, file_size=9000 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-14b, id=qwen2.5-coder-14b-instruct-generic-cpu, runtime=cpu, file_size=11325 MB, license=apache-2.0)]\n"
     ]
    }
   ],
   "source": [
    "# List available models in the catalog\n",
    "catalog = manager.list_catalog_models()\n",
    "print(f\"Available models in the catalog: {catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bfe862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 alias='phi-4' id='Phi-4-cuda-gpu' version='1' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-cuda-gpu/versions/1' file_size_mb=8570 prompt_template={'system': '<|system|>\\n{Content}<|im_end|>', 'user': '<|user|>\\n{Content}<|im_end|>', 'assistant': '<|assistant|>\\n{Content}<|im_end|>', 'prompt': '<|user|>\\n{Content}<|im_end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "2 alias='phi-4' id='Phi-4-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-generic-gpu/versions/1' file_size_mb=8570 prompt_template={'system': '<|system|>\\n{Content}<|im_end|>', 'user': '<|user|>\\n{Content}<|im_end|>', 'assistant': '<|assistant|>\\n{Content}<|im_end|>', 'prompt': '<|user|>\\n{Content}<|im_end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "3 alias='phi-4' id='Phi-4-generic-cpu' version='1' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-generic-cpu/versions/1' file_size_mb=10403 prompt_template={'system': '<|system|>\\n{Content}<|im_end|>', 'user': '<|user|>\\n{Content}<|im_end|>', 'assistant': '<|assistant|>\\n{Content}<|im_end|>', 'prompt': '<|user|>\\n{Content}<|im_end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "4 alias='phi-3-mini-128k' id='Phi-3-mini-128k-instruct-cuda-gpu' version='1' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-128k-instruct-cuda-gpu/versions/1' file_size_mb=2181 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "5 alias='phi-3-mini-128k' id='Phi-3-mini-128k-instruct-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-128k-instruct-generic-gpu/versions/1' file_size_mb=2181 prompt_template={'system': '<|system|>\\n{Content}<|end|>', 'user': '<|user|>\\n{Content}<|end|>', 'assistant': '<|assistant|>\\n{Content}<|end|>', 'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "6 alias='phi-3-mini-128k' id='Phi-3-mini-128k-instruct-generic-cpu' version='2' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-128k-instruct-generic-cpu/versions/2' file_size_mb=2600 prompt_template={'system': '<|system|>\\n{Content}<|end|>', 'user': '<|user|>\\n{Content}<|end|>', 'assistant': '<|assistant|>\\n{Content}<|end|>', 'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "7 alias='phi-3-mini-4k' id='Phi-3-mini-4k-instruct-cuda-gpu' version='1' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-4k-instruct-cuda-gpu/versions/1' file_size_mb=2181 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "8 alias='phi-3-mini-4k' id='Phi-3-mini-4k-instruct-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-4k-instruct-generic-gpu/versions/1' file_size_mb=2181 prompt_template={'system': '<|system|>\\n{Content}<|end|>', 'user': '<|user|>\\n{Content}<|end|>', 'assistant': '<|assistant|>\\n{Content}<|end|>', 'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "9 alias='phi-3-mini-4k' id='Phi-3-mini-4k-instruct-generic-cpu' version='2' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-4k-instruct-generic-cpu/versions/2' file_size_mb=2590 prompt_template={'system': '<|system|>\\n{Content}<|end|>', 'user': '<|user|>\\n{Content}<|end|>', 'assistant': '<|assistant|>\\n{Content}<|end|>', 'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "10 alias='mistral-7b-v0.2' id='mistralai-Mistral-7B-Instruct-v0-2-cuda-gpu' version='1' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/mistralai-Mistral-7B-Instruct-v0-2-cuda-gpu/versions/1' file_size_mb=4075 prompt_template={'prompt': '[INST]\\n{Content}\\n[/INST]', 'assistant': '{Content}</s>'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "11 alias='mistral-7b-v0.2' id='mistralai-Mistral-7B-Instruct-v0-2-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/mistralai-Mistral-7B-Instruct-v0-2-generic-gpu/versions/1' file_size_mb=4167 prompt_template={'prompt': '[INST]\\n{Content}\\n[/INST]', 'assistant': '{Content}</s>'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "12 alias='mistral-7b-v0.2' id='mistralai-Mistral-7B-Instruct-v0-2-generic-cpu' version='2' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/mistralai-Mistral-7B-Instruct-v0-2-generic-cpu/versions/2' file_size_mb=4167 prompt_template={'system': '<s>', 'user': '[INST]\\n{Content}\\n[/INST]', 'assistant': '{Content}</s>', 'prompt': '[INST]\\n{Content}\\n[/INST]'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "13 alias='phi-3.5-mini' id='Phi-3.5-mini-instruct-cuda-gpu' version='1' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3.5-mini-instruct-cuda-gpu/versions/1' file_size_mb=2181 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "14 alias='phi-3.5-mini' id='Phi-3.5-mini-instruct-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3.5-mini-instruct-generic-gpu/versions/1' file_size_mb=2211 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "15 alias='phi-3.5-mini' id='Phi-3.5-mini-instruct-generic-cpu' version='1' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3.5-mini-instruct-generic-cpu/versions/1' file_size_mb=2590 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "16 alias='deepseek-r1-14b' id='deepseek-r1-distill-qwen-14b-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/deepseek-r1-distill-qwen-14b-cuda-gpu/versions/3' file_size_mb=10065 prompt_template={'assistant': '{Content}', 'prompt': '\\\\u003C\\\\uFF5CUser\\\\uFF5C\\\\u003E{Content}\\\\u003C\\\\uFF5CAssistant\\\\uFF5C\\\\u003E'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "17 alias='deepseek-r1-14b' id='deepseek-r1-distill-qwen-14b-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/deepseek-r1-distill-qwen-14b-generic-gpu/versions/3' file_size_mb=10516 prompt_template={'assistant': '{Content}', 'prompt': '\\\\u003C\\\\uFF5CUser\\\\uFF5C\\\\u003E{Content}\\\\u003C\\\\uFF5CAssistant\\\\uFF5C\\\\u003E'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "18 alias='deepseek-r1-14b' id='deepseek-r1-distill-qwen-14b-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/deepseek-r1-distill-qwen-14b-generic-cpu/versions/3' file_size_mb=11786 prompt_template={'assistant': '{Content}', 'prompt': '\\\\u003C\\\\uFF5CUser\\\\uFF5C\\\\u003E{Content}\\\\u003C\\\\uFF5CAssistant\\\\uFF5C\\\\u003E'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "19 alias='deepseek-r1-7b' id='deepseek-r1-distill-qwen-7b-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/deepseek-r1-distill-qwen-7b-cuda-gpu/versions/3' file_size_mb=5406 prompt_template={'assistant': '{Content}', 'prompt': '\\\\u003C\\\\uFF5CUser\\\\uFF5C\\\\u003E{Content}\\\\u003C\\\\uFF5CAssistant\\\\uFF5C\\\\u003E'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "20 alias='deepseek-r1-7b' id='deepseek-r1-distill-qwen-7b-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/deepseek-r1-distill-qwen-7b-generic-gpu/versions/3' file_size_mb=5713 prompt_template={'assistant': '{Content}', 'prompt': '\\\\u003C\\\\uFF5CUser\\\\uFF5C\\\\u003E{Content}\\\\u003C\\\\uFF5CAssistant\\\\uFF5C\\\\u003E'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "21 alias='deepseek-r1-7b' id='deepseek-r1-distill-qwen-7b-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/deepseek-r1-distill-qwen-7b-generic-cpu/versions/3' file_size_mb=6584 prompt_template={'assistant': '{Content}', 'prompt': '\\\\u003C\\\\uFF5CUser\\\\uFF5C\\\\u003E{Content}\\\\u003C\\\\uFF5CAssistant\\\\uFF5C\\\\u003E'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "22 alias='qwen2.5-0.5b' id='qwen2.5-0.5b-instruct-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-0.5b-instruct-cuda-gpu/versions/3' file_size_mb=528 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "23 alias='qwen2.5-0.5b' id='qwen2.5-0.5b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-0.5b-instruct-generic-gpu/versions/3' file_size_mb=700 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "24 alias='qwen2.5-0.5b' id='qwen2.5-0.5b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-0.5b-instruct-generic-cpu/versions/3' file_size_mb=822 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "25 alias='qwen2.5-1.5b' id='qwen2.5-1.5b-instruct-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-1.5b-instruct-cuda-gpu/versions/3' file_size_mb=1280 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "26 alias='qwen2.5-1.5b' id='qwen2.5-1.5b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-1.5b-instruct-generic-gpu/versions/3' file_size_mb=1546 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "27 alias='qwen2.5-1.5b' id='qwen2.5-1.5b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-1.5b-instruct-generic-cpu/versions/3' file_size_mb=1822 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "28 alias='qwen2.5-coder-7b' id='qwen2.5-coder-7b-instruct-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-7b-instruct-cuda-gpu/versions/3' file_size_mb=4843 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "29 alias='qwen2.5-coder-7b' id='qwen2.5-coder-7b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-7b-instruct-generic-gpu/versions/3' file_size_mb=4843 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "30 alias='qwen2.5-coder-7b' id='qwen2.5-coder-7b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-7b-instruct-generic-cpu/versions/3' file_size_mb=6307 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "31 alias='qwen2.5-coder-0.5b' id='qwen2.5-coder-0.5b-instruct-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-0.5b-instruct-cuda-gpu/versions/3' file_size_mb=528 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "32 alias='qwen2.5-coder-0.5b' id='qwen2.5-coder-0.5b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-0.5b-instruct-generic-gpu/versions/3' file_size_mb=528 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "33 alias='qwen2.5-coder-0.5b' id='qwen2.5-coder-0.5b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-0.5b-instruct-generic-cpu/versions/3' file_size_mb=822 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "34 alias='qwen2.5-coder-1.5b' id='qwen2.5-coder-1.5b-instruct-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-1.5b-instruct-cuda-gpu/versions/3' file_size_mb=1280 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "35 alias='qwen2.5-coder-1.5b' id='qwen2.5-coder-1.5b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-1.5b-instruct-generic-gpu/versions/3' file_size_mb=1280 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "36 alias='qwen2.5-coder-1.5b' id='qwen2.5-coder-1.5b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-1.5b-instruct-generic-cpu/versions/3' file_size_mb=1822 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "37 alias='phi-4-mini' id='Phi-4-mini-instruct-cuda-gpu' version='4' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-instruct-cuda-gpu/versions/4' file_size_mb=3686 prompt_template={'system': '<|system|>{Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "38 alias='phi-4-mini' id='Phi-4-mini-instruct-generic-gpu' version='4' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-instruct-generic-gpu/versions/4' file_size_mb=3809 prompt_template={'system': '<|system|>{Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "39 alias='phi-4-mini' id='Phi-4-mini-instruct-generic-cpu' version='4' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-instruct-generic-cpu/versions/4' file_size_mb=4915 prompt_template={'system': '<|system|>{Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "40 alias='phi-4-mini-reasoning' id='Phi-4-mini-reasoning-cuda-gpu' version='2' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-reasoning-cuda-gpu/versions/2' file_size_mb=3225 prompt_template={'system': '<|system|>Your name is Phi, an AI math expert developed by Microsoft. {Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "41 alias='phi-4-mini-reasoning' id='Phi-4-mini-reasoning-generic-gpu' version='2' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-reasoning-generic-gpu/versions/2' file_size_mb=3225 prompt_template={'system': '<|system|>Your name is Phi, an AI math expert developed by Microsoft. {Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "42 alias='phi-4-mini-reasoning' id='Phi-4-mini-reasoning-generic-cpu' version='2' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-reasoning-generic-cpu/versions/2' file_size_mb=4628 prompt_template={'system': '<|system|>Your name is Phi, an AI math expert developed by Microsoft. {Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "43 alias='qwen2.5-14b' id='qwen2.5-14b-instruct-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-14b-instruct-cuda-gpu/versions/3' file_size_mb=9000 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "44 alias='qwen2.5-14b' id='qwen2.5-14b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-14b-instruct-generic-cpu/versions/3' file_size_mb=11325 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "45 alias='qwen2.5-7b' id='qwen2.5-7b-instruct-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-7b-instruct-cuda-gpu/versions/3' file_size_mb=4843 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "46 alias='qwen2.5-7b' id='qwen2.5-7b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-7b-instruct-generic-gpu/versions/3' file_size_mb=5324 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "47 alias='qwen2.5-7b' id='qwen2.5-7b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-7b-instruct-generic-cpu/versions/3' file_size_mb=6307 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "48 alias='qwen2.5-coder-14b' id='qwen2.5-coder-14b-instruct-cuda-gpu' version='3' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-14b-instruct-cuda-gpu/versions/3' file_size_mb=9000 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "49 alias='qwen2.5-coder-14b' id='qwen2.5-coder-14b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-14b-instruct-generic-gpu/versions/3' file_size_mb=9000 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "50 alias='qwen2.5-coder-14b' id='qwen2.5-coder-14b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-14b-instruct-generic-cpu/versions/3' file_size_mb=11325 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(catalog, start=1):\n",
    "    print(idx, item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28bcd51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi-4-cuda-gpu\n",
      "Phi-4-generic-gpu\n",
      "Phi-4-generic-cpu\n",
      "Phi-3-mini-128k-instruct-cuda-gpu\n",
      "Phi-3-mini-128k-instruct-generic-gpu\n",
      "Phi-3-mini-128k-instruct-generic-cpu\n",
      "Phi-3-mini-4k-instruct-cuda-gpu\n",
      "Phi-3-mini-4k-instruct-generic-gpu\n",
      "Phi-3-mini-4k-instruct-generic-cpu\n",
      "mistralai-Mistral-7B-Instruct-v0-2-cuda-gpu\n",
      "mistralai-Mistral-7B-Instruct-v0-2-generic-gpu\n",
      "mistralai-Mistral-7B-Instruct-v0-2-generic-cpu\n",
      "Phi-3.5-mini-instruct-cuda-gpu\n",
      "Phi-3.5-mini-instruct-generic-gpu\n",
      "Phi-3.5-mini-instruct-generic-cpu\n",
      "deepseek-r1-distill-qwen-14b-cuda-gpu\n",
      "deepseek-r1-distill-qwen-14b-generic-gpu\n",
      "deepseek-r1-distill-qwen-14b-generic-cpu\n",
      "deepseek-r1-distill-qwen-7b-cuda-gpu\n",
      "deepseek-r1-distill-qwen-7b-generic-gpu\n",
      "deepseek-r1-distill-qwen-7b-generic-cpu\n",
      "qwen2.5-0.5b-instruct-cuda-gpu\n",
      "qwen2.5-0.5b-instruct-generic-gpu\n",
      "qwen2.5-0.5b-instruct-generic-cpu\n",
      "qwen2.5-1.5b-instruct-cuda-gpu\n",
      "qwen2.5-1.5b-instruct-generic-gpu\n",
      "qwen2.5-1.5b-instruct-generic-cpu\n",
      "qwen2.5-coder-7b-instruct-cuda-gpu\n",
      "qwen2.5-coder-7b-instruct-generic-gpu\n",
      "qwen2.5-coder-7b-instruct-generic-cpu\n",
      "qwen2.5-coder-0.5b-instruct-cuda-gpu\n",
      "qwen2.5-coder-0.5b-instruct-generic-gpu\n",
      "qwen2.5-coder-0.5b-instruct-generic-cpu\n",
      "qwen2.5-coder-1.5b-instruct-cuda-gpu\n",
      "qwen2.5-coder-1.5b-instruct-generic-gpu\n",
      "qwen2.5-coder-1.5b-instruct-generic-cpu\n",
      "Phi-4-mini-instruct-cuda-gpu\n",
      "Phi-4-mini-instruct-generic-gpu\n",
      "Phi-4-mini-instruct-generic-cpu\n",
      "Phi-4-mini-reasoning-cuda-gpu\n",
      "Phi-4-mini-reasoning-generic-gpu\n",
      "Phi-4-mini-reasoning-generic-cpu\n",
      "qwen2.5-14b-instruct-cuda-gpu\n",
      "qwen2.5-14b-instruct-generic-cpu\n",
      "qwen2.5-7b-instruct-cuda-gpu\n",
      "qwen2.5-7b-instruct-generic-gpu\n",
      "qwen2.5-7b-instruct-generic-cpu\n",
      "qwen2.5-coder-14b-instruct-cuda-gpu\n",
      "qwen2.5-coder-14b-instruct-generic-gpu\n",
      "qwen2.5-coder-14b-instruct-generic-cpu\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(catalog)):\n",
    "    model = catalog[idx].id\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777c87bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa45ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = \"phi-3.5-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e39c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info:\n",
      "alias='phi-3.5-mini' id='Phi-3.5-mini-instruct-cuda-gpu' version='1' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3.5-mini-instruct-cuda-gpu/versions/1' file_size_mb=2181 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n"
     ]
    }
   ],
   "source": [
    "# Download and load a model\n",
    "model_info = manager.download_model(alias)\n",
    "model_info = manager.load_model(alias)\n",
    "print(f\"Model info:\\n{model_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d83fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in cache:\n",
      "[FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-cuda-gpu, runtime=cuda, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu, runtime=cuda, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-4, id=Phi-4-cuda-gpu, runtime=cuda, file_size=8570 MB, license=MIT)]\n"
     ]
    }
   ],
   "source": [
    "# List models in cache\n",
    "local_models = manager.list_cached_models()\n",
    "print(f\"Models in cache:\\n{local_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef88026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models running in the service:\n",
      "[FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu, runtime=cuda, file_size=2181 MB, license=MIT)]\n"
     ]
    }
   ],
   "source": [
    "# List loaded models\n",
    "loaded = manager.list_loaded_models()\n",
    "print(f\"Models running in the service:\\n{loaded}\")\n",
    "\n",
    "# Unload a model\n",
    "manager.unload_model(alias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95451a7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aeb996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pi (π) is a mathematical constant representing the ratio of a circle' extruded circumference to its diameter. It is an irrational number, which means it cannot be expressed as a simple fraction and its decimal representation goes on forever without repeating. Pi is approximately equal to 3.14159, but its decimal places continue infinitely without any pattern. In mathematics, pi is crucial for calculations involving circles, such as finding the area or the circumference. The area of a circle is calculated as π times the square of its radius (A = πr²), and the circumference is calculated as 2π times the radius (C = 2πr). Pi is also transcendental, meaning it is not a root of any non-zero polynomial equation with rational coefficients, which was proven by Ferdinand von Lindemann in 1882. This proof contributed to the understanding that pi cannot be the solution to any algebraic equation"
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "\n",
    "alias = \"phi-3.5-mini\"\n",
    "\n",
    "manager = FoundryLocalManager(alias)\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=manager.endpoint,\n",
    "    api_key=manager.api_key  # API key is not required for local usage\n",
    ")\n",
    "\n",
    "# Set the model to use and generate a streaming response\n",
    "stream = client.chat.completions.create(model=manager.get_model_info(alias).id,\n",
    "                                        messages=[{\n",
    "                                            \"role\": \"user\",\n",
    "                                            \"content\": \"What is pi?\"\n",
    "                                        }],\n",
    "                                        stream=True)\n",
    "\n",
    "# Print the streaming response\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfb167e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chat.id.201', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=' The Dottie number refers to a specific constant value in mathematics, particularly in the context of dynamical systems and chaos theory. It is the only real number that remains invariant under the iterative application of the logistic map at a certain parameter value. The logistic map is a polynomial mapping (equivalently, a recursive function) of degree 2, often cited as an example of how complex, chaotic behavior can arise from very simple non-linear dynamical equations.\\n\\nThe logistic map is defined as:\\n\\nx_{n+1} = r * x_n * (1 - x_n)\\n\\nHere, r is a parameter, and x_n represents the state of the system at the nth iteration.\\n\\nThe Dottie number is the stable fixed point of the logistic map when r is set to 3.7. It is the value of x for which:\\n\\nx = 3.7', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], name=None, tool_call_id=None), delta={'role': 'assistant', 'content': ' The Dottie number refers to a specific constant value in mathematics, particularly in the context of dynamical systems and chaos theory. It is the only real number that remains invariant under the iterative application of the logistic map at a certain parameter value. The logistic map is a polynomial mapping (equivalently, a recursive function) of degree 2, often cited as an example of how complex, chaotic behavior can arise from very simple non-linear dynamical equations.\\n\\nThe logistic map is defined as:\\n\\nx_{n+1} = r * x_n * (1 - x_n)\\n\\nHere, r is a parameter, and x_n represents the state of the system at the nth iteration.\\n\\nThe Dottie number is the stable fixed point of the logistic map when r is set to 3.7. It is the value of x for which:\\n\\nx = 3.7', 'name': None, 'tool_call_id': None, 'function_call': None, 'tool_calls': []}, finish_details=None)], created=1749472440, model=None, object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, CreatedAt='2025-06-09T12:34:00+00:00', StreamEvent=None, IsDelta=False, Successful=True, error=None, HttpStatusCode=0, HeaderValues=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=manager.get_model_info(alias).id,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the Dottie number?\"\n",
    "    }],\n",
    ")\n",
    "\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36411bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Dottie number refers to a specific constant value in mathematics, particularly in the context of dynamical systems and chaos theory. It is the only real number that remains invariant under the iterative application of the logistic map at a certain parameter value. The logistic map is a polynomial mapping (equivalently, a recursive function) of degree 2, often cited as an example of how complex, chaotic behavior can arise from very simple non-linear dynamical equations.\n",
      "\n",
      "The logistic map is defined as:\n",
      "\n",
      "x_{n+1} = r * x_n * (1 - x_n)\n",
      "\n",
      "Here, r is a parameter, and x_n represents the state of the system at the nth iteration.\n",
      "\n",
      "The Dottie number is the stable fixed point of the logistic map when r is set to 3.7. It is the value of x for which:\n",
      "\n",
      "x = 3.7\n"
     ]
    }
   ],
   "source": [
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9251f",
   "metadata": {},
   "source": [
    "## Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d804dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = \"phi-4\"\n",
    "\n",
    "manager = FoundryLocalManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a311fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info:\n",
      "alias='phi-4' id='Phi-4-cuda-gpu' version='1' runtime=<ExecutionProvider.CUDA: 'CUDAExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-cuda-gpu/versions/1' file_size_mb=8570 prompt_template={'system': '<|system|>\\n{Content}<|im_end|>', 'user': '<|user|>\\n{Content}<|im_end|>', 'assistant': '<|assistant|>\\n{Content}<|im_end|>', 'prompt': '<|user|>\\n{Content}<|im_end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n"
     ]
    }
   ],
   "source": [
    "# Download and load a model\n",
    "model_info = manager.download_model(alias)\n",
    "model_info = manager.load_model(alias)\n",
    "print(f\"Model info:\\n{model_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3ad7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in cache:\n",
      "[FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-cuda-gpu, runtime=cuda, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu, runtime=cuda, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-4, id=Phi-4-cuda-gpu, runtime=cuda, file_size=8570 MB, license=MIT)]\n"
     ]
    }
   ],
   "source": [
    "# List models in cache\n",
    "local_models = manager.list_cached_models()\n",
    "print(f\"Models in cache:\\n{local_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2e3c526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models running in the service:\n",
      "[FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu, runtime=cuda, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-4, id=Phi-4-cuda-gpu, runtime=cuda, file_size=8570 MB, license=MIT)]\n"
     ]
    }
   ],
   "source": [
    "# List loaded models\n",
    "loaded = manager.list_loaded_models()\n",
    "print(f\"Models running in the service:\\n{loaded}\")\n",
    "\n",
    "# Unload a model\n",
    "manager.unload_model(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aaa9b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "alias = \"phi-4\"\n",
    "\n",
    "manager = FoundryLocalManager(alias)\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=manager.endpoint,\n",
    "    api_key=manager.api_key  # API key is not required for local usage\n",
    ")\n",
    "\n",
    "# Set the model to use and generate a streaming response\n",
    "stream = client.chat.completions.create(model=manager.get_model_info(alias).id,\n",
    "                                        messages=[{\n",
    "                                            \"role\":\n",
    "                                            \"user\",\n",
    "                                            \"content\":\n",
    "                                            \"What is the capital of France?\"\n",
    "                                        }],\n",
    "                                        stream=True)\n",
    "\n",
    "# Print the streaming response\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee53e0",
   "metadata": {},
   "source": [
    "## Rest API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d261682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm Phi, an AI developed by Microsoft. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "alias = \"phi-3.5-mini\"\n",
    "\n",
    "manager = FoundryLocalManager(alias)\n",
    "url = manager.endpoint + \"/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": manager.get_model_info(alias).id,\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello\",\n",
    "    }]\n",
    "}\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "177055da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': None,\n",
       " 'choices': [{'delta': {'role': 'assistant',\n",
       "    'content': \" Hello! I'm Phi, an AI developed by Microsoft. How can I help you today?\",\n",
       "    'name': None,\n",
       "    'tool_call_id': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': []},\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \" Hello! I'm Phi, an AI developed by Microsoft. How can I help you today?\",\n",
       "    'name': None,\n",
       "    'tool_call_id': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': []},\n",
       "   'index': 0,\n",
       "   'finish_reason': 'stop',\n",
       "   'finish_details': None,\n",
       "   'logprobs': None}],\n",
       " 'usage': None,\n",
       " 'system_fingerprint': None,\n",
       " 'service_tier': None,\n",
       " 'created': 1749472492,\n",
       " 'CreatedAt': '2025-06-09T12:34:52+00:00',\n",
       " 'id': 'chat.id.210',\n",
       " 'StreamEvent': None,\n",
       " 'IsDelta': False,\n",
       " 'Successful': True,\n",
       " 'error': None,\n",
       " 'HttpStatusCode': 0,\n",
       " 'HeaderValues': None,\n",
       " 'object': 'chat.completion'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184eeb0",
   "metadata": {},
   "source": [
    "## CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66353390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:\n",
      "  Foundry Local CLI: Run AI models on your device.\n",
      "  \n",
      "  ðŸš€ Getting started:\n",
      "  \n",
      "     1. To view available models: foundry model list\n",
      "     2. To run a model: foundry model run <model>\n",
      "  \n",
      "     EXAMPLES:\n",
      "         foundry model run phi-3-mini-4k\n",
      "\n",
      "UtilisationÂ :\n",
      "  foundry [command] [options]\n",
      "\n",
      "OptionsÂ :\n",
      "  -?, -h, --help  Show help and usage information\n",
      "  --version       Afficher les informations de version\n",
      "  --license       Display foudry license information\n",
      "\n",
      "CommandesÂ :\n",
      "  model    Discover, run and manage models\n",
      "  cache    Manage the local cache\n",
      "  service  Manage the local model inference service\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!foundry -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "621d6d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.91+269dfd9ed1\n"
     ]
    }
   ],
   "source": [
    "!foundry --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e54e917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alias                          Device     Task               File Size    License      Model ID            \n",
      "-----------------------------------------------------------------------------------------------\n",
      "phi-4                          GPU        chat-completion    8.37 GB      MIT          Phi-4-cuda-gpu      \n",
      "                               GPU        chat-completion    8.37 GB      MIT          Phi-4-generic-gpu   \n",
      "                               CPU        chat-completion    10.16 GB     MIT          Phi-4-generic-cpu   \n",
      "--------------------------------------------------------------------------------------------------------\n",
      "phi-3-mini-128k                GPU        chat-completion    2.13 GB      MIT          Phi-3-mini-128k-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    2.13 GB      MIT          Phi-3-mini-128k-instruct-generic-gpu\n",
      "                               CPU        chat-completion    2.54 GB      MIT          Phi-3-mini-128k-instruct-generic-cpu\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "phi-3-mini-4k                  GPU        chat-completion    2.13 GB      MIT          Phi-3-mini-4k-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    2.13 GB      MIT          Phi-3-mini-4k-instruct-generic-gpu\n",
      "                               CPU        chat-completion    2.53 GB      MIT          Phi-3-mini-4k-instruct-generic-cpu\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "mistral-7b-v0.2                GPU        chat-completion    3.98 GB      apache-2.0   mistralai-Mistral-7B-Instruct-v0-2-cuda-gpu\n",
      "                               GPU        chat-completion    4.07 GB      apache-2.0   mistralai-Mistral-7B-Instruct-v0-2-generic-gpu\n",
      "                               CPU        chat-completion    4.07 GB      apache-2.0   mistralai-Mistral-7B-Instruct-v0-2-generic-cpu\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "phi-3.5-mini                   GPU        chat-completion    2.13 GB      MIT          Phi-3.5-mini-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    2.16 GB      MIT          Phi-3.5-mini-instruct-generic-gpu\n",
      "                               CPU        chat-completion    2.53 GB      MIT          Phi-3.5-mini-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "deepseek-r1-14b                GPU        chat-completion    9.83 GB      MIT          deepseek-r1-distill-qwen-14b-cuda-gpu\n",
      "                               GPU        chat-completion    10.27 GB     MIT          deepseek-r1-distill-qwen-14b-generic-gpu\n",
      "                               CPU        chat-completion    11.51 GB     MIT          deepseek-r1-distill-qwen-14b-generic-cpu\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "deepseek-r1-7b                 GPU        chat-completion    5.28 GB      MIT          deepseek-r1-distill-qwen-7b-cuda-gpu\n",
      "                               GPU        chat-completion    5.58 GB      MIT          deepseek-r1-distill-qwen-7b-generic-gpu\n",
      "                               CPU        chat-completion    6.43 GB      MIT          deepseek-r1-distill-qwen-7b-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-0.5b                   GPU        chat-completion    0.52 GB      apache-2.0   qwen2.5-0.5b-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    0.68 GB      apache-2.0   qwen2.5-0.5b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    0.80 GB      apache-2.0   qwen2.5-0.5b-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-1.5b                   GPU        chat-completion    1.25 GB      apache-2.0   qwen2.5-1.5b-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    1.51 GB      apache-2.0   qwen2.5-1.5b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    1.78 GB      apache-2.0   qwen2.5-1.5b-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-coder-7b               GPU        chat-completion    4.73 GB      apache-2.0   qwen2.5-coder-7b-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    4.73 GB      apache-2.0   qwen2.5-coder-7b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    6.16 GB      apache-2.0   qwen2.5-coder-7b-instruct-generic-cpu\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-coder-0.5b             GPU        chat-completion    0.52 GB      apache-2.0   qwen2.5-coder-0.5b-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    0.52 GB      apache-2.0   qwen2.5-coder-0.5b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    0.80 GB      apache-2.0   qwen2.5-coder-0.5b-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-coder-1.5b             GPU        chat-completion    1.25 GB      apache-2.0   qwen2.5-coder-1.5b-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    1.25 GB      apache-2.0   qwen2.5-coder-1.5b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    1.78 GB      apache-2.0   qwen2.5-coder-1.5b-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "phi-4-mini                     GPU        chat-completion    3.60 GB      MIT          Phi-4-mini-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    3.72 GB      MIT          Phi-4-mini-instruct-generic-gpu\n",
      "                               CPU        chat-completion    4.80 GB      MIT          Phi-4-mini-instruct-generic-cpu\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "phi-4-mini-reasoning           GPU        chat-completion    3.15 GB      MIT          Phi-4-mini-reasoning-cuda-gpu\n",
      "                               GPU        chat-completion    3.15 GB      MIT          Phi-4-mini-reasoning-generic-gpu\n",
      "                               CPU        chat-completion    4.52 GB      MIT          Phi-4-mini-reasoning-generic-cpu\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-14b                    GPU        chat-completion    8.79 GB      apache-2.0   qwen2.5-14b-instruct-cuda-gpu\n",
      "                               CPU        chat-completion    11.06 GB     apache-2.0   qwen2.5-14b-instruct-generic-cpu\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-7b                     GPU        chat-completion    4.73 GB      apache-2.0   qwen2.5-7b-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    5.20 GB      apache-2.0   qwen2.5-7b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    6.16 GB      apache-2.0   qwen2.5-7b-instruct-generic-cpu\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-coder-14b              GPU        chat-completion    8.79 GB      apache-2.0   qwen2.5-coder-14b-instruct-cuda-gpu\n",
      "                               GPU        chat-completion    8.79 GB      apache-2.0   qwen2.5-coder-14b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    11.06 GB     apache-2.0   qwen2.5-coder-14b-instruct-generic-cpu\n"
     ]
    }
   ],
   "source": [
    "!foundry model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93981275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alias                          Device     Task               File Size    License      Model ID            \n",
      "phi-4-mini-reasoning           GPU        chat-completion    3.15 GB      MIT          Phi-4-mini-reasoning-cuda-gpu\n"
     ]
    }
   ],
   "source": [
    "!foundry model info phi-4-mini-reasoning   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71c01724",
   "metadata": {},
   "outputs": [],
   "source": [
    "!foundry service restart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bf2f7",
   "metadata": {},
   "source": [
    "## Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14a76559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.is_service_running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e95bd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:5273'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.service_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d93ce33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:5273/v1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0ba62da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.list_loaded_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe21281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
