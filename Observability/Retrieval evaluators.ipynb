{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b758c11-7281-4db6-92c8-97054757001c",
   "metadata": {},
   "source": [
    "# Retrieval evaluators\n",
    "\n",
    "**A retrieval-augmented generation (RAG)** system tries to generate the most relevant answer consistent with grounding documents in response to a user's query. At a high level, a user's query triggers a search retrieval in the corpus of grounding documents to provide grounding context for the AI model to generate a response. It's important to evaluate:\n",
    "\n",
    "- The relevance of the retrieval results to the user's query: use Document Retrieval if you have labels for query-specific document relevance, or query relevance judgement (qrels) for more accurate measurements. Use Retrieval if you only have the retrieved context, but you don't have such labels and have a higher tolerance for a less fine-grained measurement.\n",
    "- The consistency of the generated response with respect to the grounding documents: use Groundedness if you want to potentially customize the definition of groundedness in our open-source LLM-judge prompt, Groundedness Pro if you want a straightforward definition.\n",
    "- The relevance of the final response to the query: Relevance if you don't have ground truth, and Response Completeness if you have ground truth and don't want your response to miss critical information.\n",
    "  \n",
    "A good way to think about Groundedness and Response Completeness is: groundedness is about the precision aspect of the response that it shouldn't contain content outside of the grounding context, whereas response completeness is about the recall aspect of the response that it shouldn't miss critical information compared to the expected response (ground truth).\n",
    "\n",
    "> https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/rag-evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c169c4eb-50d9-404d-b07d-c25ca6dc65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration, DocumentRetrievalEvaluator, GroundednessEvaluator, GroundednessProEvaluator, RelevanceEvaluator, ResponseCompletenessEvaluator, RetrievalEvaluator\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92b928e-e814-41b1-83bb-178c3ea32487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08150cfd-c489-4159-b9b8-a9c9441bf58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 26-Jun-2025 12:30:11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Today is {datetime.datetime.today().strftime('%d-%b-%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315d6c18-4ec7-482e-84bb-17473661ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"azure.env\")\n",
    "\n",
    "endpoint = os.getenv(\"endpoint\")\n",
    "key = os.getenv(\"key\")\n",
    "azure_foundry_project = os.environ.get(\"azure_foundry_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19eada90-be10-4a8f-8af6-28f943551783",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=key,\n",
    "    azure_deployment=\"gpt-4.1\",\n",
    "    api_version=\"2024-10-21\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba651fb8-d74e-401e-a6cc-1371ee1d41fd",
   "metadata": {},
   "source": [
    "## Retrieval evaluator\n",
    "\n",
    "> Measures how effectively the system retrieves relevant information.\n",
    "\n",
    "Retrieval quality is very important given its upstream role in RAG: if the retrieval quality is poor and the response requires corpus-specific knowledge, there's less chance your LLM model gives you a satisfactory answer. RetrievalEvaluator measures the textual quality of retrieval results with an LLM without requiring ground truth (also known as query relevance judgment), which provides value compared to DocumentRetrievalEvaluator measuring ndcg, xdcg, fidelity, and other classical information retrieval metrics that require ground truth. This metric focuses on how relevant the context chunks (encoded as a string) are to address a query and how the most relevant context chunks are surfaced at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037dc8fe-a9bc-4ae5-a4a0-cb1d32b28c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_evaluator = RetrievalEvaluator(model_config=model_config, threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8fa7b49-3b97-40c0-96f7-c3ea20b24737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'retrieval': 5.0,\n",
       " 'gpt_retrieval': 5.0,\n",
       " 'retrieval_reason': 'The context directly and immediately answers the query with the most relevant information at the top, with no external knowledge or bias introduced.',\n",
       " 'retrieval_result': 'pass',\n",
       " 'retrieval_threshold': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_evaluator(\n",
    "    query=\"Where was Marie Curie born?\",\n",
    "    context=\n",
    "    \"Background: 1. Marie Curie was born in Warsaw. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487489c0-d665-4b91-9890-9f038f2c3f06",
   "metadata": {},
   "source": [
    "## Document retrieval evaluator\n",
    "\n",
    "> Measures accuracy in retrieval results given ground truth.\n",
    "\n",
    "Retrieval quality is very important given its upstream role in RAG: if the retrieval quality is poor and the response requires corpus-specific knowledge, there's less chance your LLM model gives you a satisfactory answer. Therefore, it's important to use DocumentRetrievalEvaluator to evaluate the retrieval quality but also optimize your search parameters for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03ce019-4cc4-4847-9620-0a5c8bed711c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg@3': 0.31075932533963707,\n",
       " 'xdcg@3': 39.285714285714285,\n",
       " 'fidelity': 0.39285714285714285,\n",
       " 'top1_relevance': 2,\n",
       " 'top3_max_relevance': 3,\n",
       " 'holes': 2,\n",
       " 'holes_ratio': 0.4,\n",
       " 'total_retrieved_documents': 5,\n",
       " 'total_ground_truth_documents': 5,\n",
       " 'ndcg@3_result': 'fail',\n",
       " 'ndcg@3_threshold': 0.5,\n",
       " 'ndcg@3_higher_is_better': True,\n",
       " 'xdcg@3_result': 'fail',\n",
       " 'xdcg@3_threshold': 50.0,\n",
       " 'xdcg@3_higher_is_better': True,\n",
       " 'fidelity_result': 'fail',\n",
       " 'fidelity_threshold': 0.5,\n",
       " 'fidelity_higher_is_better': True,\n",
       " 'top1_relevance_result': 'fail',\n",
       " 'top1_relevance_threshold': 50.0,\n",
       " 'top1_relevance_higher_is_better': True,\n",
       " 'top3_max_relevance_result': 'fail',\n",
       " 'top3_max_relevance_threshold': 50.0,\n",
       " 'top3_max_relevance_higher_is_better': True,\n",
       " 'holes_result': 'fail',\n",
       " 'holes_threshold': 0,\n",
       " 'holes_higher_is_better': False,\n",
       " 'holes_ratio_result': 'fail',\n",
       " 'holes_ratio_threshold': 0,\n",
       " 'holes_ratio_higher_is_better': False,\n",
       " 'total_retrieved_documents_result': 'fail',\n",
       " 'total_retrieved_documents_threshold': 50,\n",
       " 'total_retrieved_documents_higher_is_better': True,\n",
       " 'total_ground_truth_documents_result': 'fail',\n",
       " 'total_ground_truth_documents_threshold': 50,\n",
       " 'total_ground_truth_documents_higher_is_better': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these query_relevance_label are given by your human- or LLM-judges.\n",
    "retrieval_ground_truth = [\n",
    "    {\n",
    "        \"document_id\": \"1\",\n",
    "        \"query_relevance_label\": 4\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"2\",\n",
    "        \"query_relevance_label\": 2\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"3\",\n",
    "        \"query_relevance_label\": 3\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"4\",\n",
    "        \"query_relevance_label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"5\",\n",
    "        \"query_relevance_label\": 0\n",
    "    },\n",
    "]\n",
    "# the min and max of the label scores are inputs to document retrieval evaluator\n",
    "ground_truth_label_min = 0\n",
    "ground_truth_label_max = 4\n",
    "\n",
    "# these relevance scores come from your search retrieval system\n",
    "retrieved_documents = [\n",
    "    {\n",
    "        \"document_id\": \"2\",\n",
    "        \"relevance_score\": 45.1\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"6\",\n",
    "        \"relevance_score\": 35.8\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"3\",\n",
    "        \"relevance_score\": 29.2\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"5\",\n",
    "        \"relevance_score\": 25.4\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"7\",\n",
    "        \"relevance_score\": 18.8\n",
    "    },\n",
    "]\n",
    "\n",
    "document_retrieval_evaluator = DocumentRetrievalEvaluator(\n",
    "    ground_truth_label_min=ground_truth_label_min,\n",
    "    ground_truth_label_max=ground_truth_label_max,\n",
    "    ndcg_threshold=0.5,\n",
    "    xdcg_threshold=50.0,\n",
    "    fidelity_threshold=0.5,\n",
    "    top1_relevance_threshold=50.0,\n",
    "    top3_max_relevance_threshold=50.0,\n",
    "    total_retrieved_documents_threshold=50,\n",
    "    total_ground_truth_documents_threshold=50)\n",
    "\n",
    "document_retrieval_evaluator(retrieval_ground_truth=retrieval_ground_truth,\n",
    "                             retrieved_documents=retrieved_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe034f84-618b-40e3-9abc-e8cef375e515",
   "metadata": {},
   "source": [
    "## Groundedness Evaluator\n",
    "\n",
    "> Measures how consistent the response is with respect to the retrieved context.\n",
    "\n",
    "It's important to evaluate how grounded the response is in relation to the context, because AI models can fabricate content or generate irrelevant responses. GroundednessEvaluator measures how well the generated response aligns with the given context (grounding source) and doesn't fabricate content outside of it. This metric captures the precision aspect of response alignment with the grounding source. Lower score means the response is irrelevant to the query or fabricated inaccurate content outside the context. This metric is complementary to ResponseCompletenessEvaluator that captures the recall aspect of response alignment with the expected response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc933c8-96a3-49cb-8567-060b14a61ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groundedness': 5.0,\n",
       " 'gpt_groundedness': 5.0,\n",
       " 'groundedness_reason': 'The response is fully correct and complete, directly using the context to answer the query.',\n",
       " 'groundedness_result': 'pass',\n",
       " 'groundedness_threshold': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundedness_evaluator = GroundednessEvaluator(\n",
    "    model_config=model_config, threshold=3)\n",
    "\n",
    "groundedness_evaluator(\n",
    "    query=\"Is Marie Curie is born in Paris?\",\n",
    "    context=\"Background: 1. Marie Curie is born on November 7, 1867. 2. Marie Curie is born in Warsaw.\",\n",
    "    response=\"No, Marie Curie is born in Warsaw.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f53a60-c71e-40b7-b711-364ca08fa528",
   "metadata": {},
   "source": [
    "## Groundedness ProEvaluator\n",
    "\n",
    "> Measures whether the response is consistent with respect to the retrieved context.\n",
    "\n",
    "AI systems can fabricate content or generate irrelevant responses outside the given context. Powered by Azure AI Content Safety, GroundednessProEvaluator detects whether the generated text response is consistent or accurate with respect to the given context in a retrieval-augmented generation question-and-answering scenario. It checks whether the response adheres closely to the context in order to answer the query, avoiding speculation or fabrication, and outputs a binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fad4068-933b-403e-a827-c1790f6fac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class GroundednessProEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "groundedness_pro_evaluator = GroundednessProEvaluator(\n",
    "    azure_ai_project=azure_foundry_project,\n",
    "    credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f230f5-eb1e-4625-a337-6f3c4f19613f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groundedness_pro_reason': 'All Contents are grounded',\n",
       " 'groundedness_pro_label': True,\n",
       " 'groundedness_pro_score': 1,\n",
       " 'groundedness_pro_threshold': 5,\n",
       " 'groundedness_pro_result': 'pass'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundedness_pro_evaluator(\n",
    "    query=\"Is Marie Curie is born in Paris?\",\n",
    "    context=\n",
    "    \"Background: 1. Marie Curie is born on November 7, 1867. 2. Marie Curie is born in Warsaw.\",\n",
    "    response=\"No, Marie Curie is born in Warsaw.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0015fd9-7e72-4656-a242-a0d4b4858c38",
   "metadata": {},
   "source": [
    "## Relevance Evaluator\n",
    "> Measures how relevant the response is with respect to the query.\n",
    "\n",
    "It's important to evaluate the final response because AI models can generate irrelevant responses with respect to a user query. To address this, you can use RelevanceEvaluator which measures how effectively a response addresses a query. It assesses the accuracy, completeness, and direct relevance of the response based solely on the given query. Higher scores mean better relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acbe4281-a650-48d2-921e-5ecae4a5dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_evaluator = RelevanceEvaluator(model_config=model_config, threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcf72bc8-9ce5-43c6-8ded-4bc22e1109e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevance': 4.0,\n",
       " 'gpt_relevance': 4.0,\n",
       " 'relevance_reason': 'The response fully and accurately answers the question, providing all essential details for understanding.',\n",
       " 'relevance_result': 'pass',\n",
       " 'relevance_threshold': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_evaluator(query=\"Is Marie Curie is born in Paris?\",\n",
    "                    response=\"No, Marie Curie is born in Warsaw.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283ffae-4604-403d-8840-0b9981fefd3f",
   "metadata": {},
   "source": [
    "## Response Completeness Evaluator\n",
    "\n",
    "> Measures to what extent the response is complete (not missing critical information) with respect to the ground truth.\n",
    "\n",
    "AI systems can fabricate content or generate irrelevant responses outside the given context. Given ground truth response, ResponseCompletenessEvaluator that captures the recall aspect of response alignment with the expected response. This is complementary to GroundednessEvaluator which captures the precision aspect of response alignment with the grounding source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "465c6b5e-3fa5-43ff-ad58-1b3694f2ecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ResponseCompletenessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "response_completeness_evaluator = ResponseCompletenessEvaluator(model_config=model_config, threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cae2a94-11e8-4bc3-94c5-0746f35dda17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 1,\n",
       " 'response_completeness_result': 'fail',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': \"The response does not include any information about the CEO's compensation package, which is the sole topic in the ground truth. Therefore, it is fully incomplete.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_completeness_evaluator(\n",
    "    response=\n",
    "    \"Based on the retrieved documents, the shareholder meeting discussed the operational efficiency of the company and financing options.\",\n",
    "    ground_truth=\n",
    "    \"The shareholder meeting discussed the compensation package of the company CEO.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f770e83-9df1-4fc6-adf5-8fb4744a7e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
